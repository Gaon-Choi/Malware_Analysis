import json
import os
import pandas as pd
from tqdm import tqdm

def parse(folder_path):
    base_path = os.path.join(folder_path, 'base.json')
    dropped_files_path = os.path.join(folder_path, 'dropped_files.json')
    behaviours_path = os.path.join(folder_path, 'behaviours.json')
    contacted_domains_path = os.path.join(folder_path, 'contacted_domains.json')
    contacted_ips_path = os.path.join(folder_path, 'contacted_ips.json')
    contacted_urls_path = os.path.join(folder_path, 'contacted_urls.json')

    with open(base_path, 'r', encoding='utf-8') as f:
        base = json.load(f)
    with open(dropped_files_path, 'r', encoding='utf-8') as f:
        dropped_files = json.load(f)
    with open(behaviours_path, 'r', encoding='utf-8') as f:
        behaviours = json.load(f)
    with open(contacted_domains_path, 'r', encoding='utf-8') as f:
        contacted_domains = json.load(f)
    with open(contacted_ips_path, 'r', encoding='utf-8') as f:
        contacted_ips = json.load(f)
    with open(contacted_urls_path, 'r', encoding='utf-8') as f:
        contacted_urls = json.load(f)

    result = {}

    base_attribute = base['data']['attributes']
    result['type_description'] = base_attribute['type_description']
    result['best_trid_type'] = base_attribute['trid'][0]['file_type'] if 'trid' in base_attribute else 'unknown'
    result['best_trid_probability'] = base_attribute['trid'][0]['probability'] if 'trid' in base_attribute else 0
    result['type_extension'] = base_attribute['type_extension'] if 'type_extension' in base_attribute else 'unknown'
    result['has_signature'] = 1 if 'signature_info' in base_attribute else 0
    result['packer'] = list(base_attribute['packers'].values())[0] if 'packers' in base_attribute else 'unknown'
    result['type_tag'] = base_attribute['type_tag'] 

    if 'pe_info' in base_attribute and 'overlay' in base_attribute['pe_info']:
        pe_info = base_attribute['pe_info']
        overlay = pe_info['overlay']
        result['overlay_entropy'] = overlay['entropy']
        result['overlay_chi2'] = overlay['chi2']
        result['overlay_filetype'] = overlay['filetype']
    else:
        result['overlay_entropy'] = 0
        result['overlay_chi2'] = 0
        result['overlay_filetype'] = 'unknown'

    
    # dropped_files
    try:
        result['dropped_files_count'] = dropped_files['meta']['count']
    except:
        print(dropped_files)

    # behaviours_count
    try:
        result['behaviours_count'] = behaviours['meta']['count']
    except:
        print(behaviours)

    # contacted_domains_count
    try:
        result['contacted_domains_count'] = contacted_domains['meta']['count']
    except:
        print(contacted_domains)
        
    # contacted_ips_count
    try:
        result['contacted_ips_count'] = contacted_ips['meta']['count']
    except:
        print(contacted_ips)
        
    # contacted_urls_count
    try:
        result['contacted_urls_count'] = contacted_urls['meta']['count']
    except:
        print(contacted_urls)

    sections = [
        '.text',
        '.bss',
        '.rdata',
        '.data',
        '.xdata',
        '.idata',
        '.pdata',
        '.rsrc',
        '.reloc',
        '.CRT',
    ]

    for section in sections:
        result[section + '_entropy'] = 0

    if 'pe_info' in base_attribute and 'sections' in base_attribute['pe_info']:
        for section in base_attribute['pe_info']['sections']:
            if section['name'] in sections:
                result[section['name'] + '_entropy'] = section['entropy']

    
    return result

def main():
    train_df = pd.read_csv('./mdataset/1.trainSet.csv', names=['hash', 'result'])
    pre_df = pd.read_csv('./mdataset/2.preSet.csv', names=['hash', 'result'])
    final1_df = pd.read_csv('./mdataset/3.finalSet1.csv', names=['hash', 'result'])
    final2_df = pd.read_csv('./mdataset/4.finalSet2.csv', names=['hash', 'result'])


    dfs = [train_df, pre_df, final1_df, final2_df]
    new_dfs_paths = [
        './dataset/train.csv',
        './dataset/valid.csv',
        './dataset/test1.csv',
        './dataset/test2.csv',
    ]

    for i in dfs:
        i['hash'] = i['hash'].apply(lambda x: x.split('.')[0])

    valid_hash_list = os.listdir('./fetch_result')
    for h in final2_df['hash']:
        if h not in valid_hash_list:
            print(h)
    print(len(valid_hash_list))
    dfs = [i[i['hash'].isin(valid_hash_list)] for i in dfs]
    d = {}
    for df, new_df_path in zip(dfs, new_dfs_paths):
        results = []
        for h, is_mal in tqdm(df.values):
            try:
                path = f'./fetch_result/{h}'
                result = parse(path)
                result['hash'] = h
                result['is_mal'] = is_mal
                results.append(result)
            except:
                pass
            # r = result['sections_count']
            # if r in d:
            #     d[r] += 1
            # else:
            #     d[r] = 1
        new_df = pd.DataFrame(results)
        new_df.to_csv(new_df_path, index=False)
        print(new_df)
    print(d)


    print(*[len(i) for i in dfs])
    

if __name__ == '__main__':
    main()