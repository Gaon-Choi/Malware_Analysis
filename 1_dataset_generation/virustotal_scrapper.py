import pandas as pd
import os
import base64
from seleniumwire import webdriver
from seleniumwire.utils import decode
import chromedriver_autoinstaller 
import json
from urllib.parse import urlparse
import shutil
import traceback
from tqdm import tqdm
import time
chromedriver_autoinstaller.install()

def rate_limiter(call_per_hour):
    call_per_sec = call_per_hour / 60 / 60
    interval_time = 1 / call_per_sec

    print(f'rate limiter: cph = {call_per_hour}, cps = {call_per_sec}, interval = {interval_time}')

    def decorate(func):
        last_called = 0

        def inner(*args, **kargs):
            nonlocal last_called
            elapsed = time.perf_counter() - last_called
            gap = interval_time - elapsed
            if gap > 0:
                time.sleep(gap)
            last_called = time.perf_counter()
            return func(*args, **kargs)
        return inner
    return decorate


def base64encode(text):
    return base64.b64encode(text.encode('ascii')).decode('ascii')

@rate_limiter(950)
def fetch(driver, file_hash, fetch_result_folder):
    url = f'https://www.virustotal.com/gui/file/{file_hash}'
    folder = os.path.join(fetch_result_folder, file_hash)

    driver.backend.storage.clear_requests()
    driver.get(url)
    driver.wait_for_request('/ui/search')
    
    for request in driver.requests:
        if 'ui/search' in request.url and request.response:
            string = decode(request.response.body, request.response.headers.get('Content-Encoding', 'identity'))
            data = json.loads(string)
            if 'data' not in data:
                raise Exception('capcha')
            if not data['data']:
                print(f'{file_hash}: no data')
                if os.path.exists(folder):
                    shutil.rmtree(folder)
                os.mkdir(folder)
                with open(os.path.join(folder, f'empty'), 'w') as f:
                    return

    driver.wait_for_request(r'\/ui\/files\/([A-Z0-9a-z])+$')
    driver.wait_for_request('/contacted_ips')
    driver.wait_for_request('/dropped_files')
    driver.wait_for_request('/execution_parents')
    driver.wait_for_request('/bundled_files')
    driver.wait_for_request('/contacted_domains')
    driver.wait_for_request('/behaviours')
    driver.wait_for_request('/contacted_urls')

    files = {}
    pattern = f'/ui/files/'
    for request in driver.requests:
        if pattern in request.url and request.response:
            uri = urlparse(request.url)
            path = uri.path
            splited_path = path.split('/')
            if len(splited_path) == 4:
                data_type = 'base'
            else:
                data_type = splited_path[4]
            
            files[data_type] = decode(request.response.body, request.response.headers.get('Content-Encoding', 'identity'))
    if 'base' not in files:
        print(list(files.keys()))
        raise Exception('failure')
    
    if os.path.exists(folder):
        shutil.rmtree(folder)
    os.mkdir(folder)
    for data_type, data in files.items():
        with open(os.path.join(folder, f'{data_type}.json'), 'w') as f:
            json.dump(json.loads(data), f)
    
def main():
    train_hash_list = pd.read_csv('./mdataset/1.trainSet.csv', names=['hash', 'result'])['hash'].to_list()
    pre_hash_list = pd.read_csv('./mdataset/2.preSet.csv', names=['hash', 'result'])['hash'].to_list()
    final1_hash_list = pd.read_csv('./mdataset/3.finalSet1.csv', names=['hash', 'result'])['hash'].to_list()
    final2_hash_list = pd.read_csv('./mdataset/4.finalSet2.csv', names=['hash', 'result'])['hash'].to_list()

    hash_list = train_hash_list + pre_hash_list + final1_hash_list + final2_hash_list

    hash_list = [i.split('.')[0] for i in hash_list]
    hash_list = list(set(hash_list))

    fetch_result_folder = 'fetch_result'
    if not os.path.exists(fetch_result_folder):
        os.mkdir(fetch_result_folder)

    if not os.path.exists('progress.json'):
        progress = {
            'todo': hash_list,
            'success': [],
            'failure': []
        }
        with open('progress.json', 'w') as f:
            json.dump(progress, f)
    else:
        with open('progress.json', 'r') as f:
            progress = json.load(f)
    driver = webdriver.Chrome()
    count = 0

    pbar = tqdm(total=len(progress['todo']))

    while len(progress['todo']) > 0:
        try:
            file_hash = progress['todo'].pop()
            try:
                fetch(driver, file_hash, fetch_result_folder)
            except Exception:
                traceback.print_exc()
                print('error!')
                input('captch. continue?')
                progress['failure'].append(file_hash)
            else:
                progress['success'].append(file_hash)
            with open('progress.json', 'w') as f:
                json.dump(progress, f)
            count += 1
            if count > 500:
                count = 0
                driver.close()
                driver = webdriver.Chrome()

            pbar.set_description(file_hash)
            pbar.update(1)
        except KeyboardInterrupt:
            re = input('continue? (y/n)')
            if re == 'n':
                break

if __name__ == '__main__':
    main()