import numpy as np
import wandb
import random
from matplotlib import legend
import os
import datetime
import torchvision.models as models
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import RandomOverSampler
import matplotlib.pyplot as plt
import pickle
import torch
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, TensorDataset
from tqdm import tqdm
import torch.nn.functional as F

import torch.nn as nn

from torchvision.datasets import ImageFolder
from torch.utils.data import DataLoader

from torchvision.transforms.transforms import Resize
transform = transforms.Compose(
    [
        transforms.Grayscale(num_output_channels=3),
        transforms.ToTensor()
    ]
)

img_path = './img'

train_data = ImageFolder(img_path, transform=transform)
train_data_list = list(train_data)


def build_label_table():
    rows = []
    for label in os.listdir(img_path):
        for img in os.listdir(os.path.join(img_path, label)):
            rows.append({
                'label': label,
                'hash': img.split('.')[0]
            })


indices = list(range(len(train_data_list)))
train_indices, test_indices = train_test_split(indices, test_size=.3)


X = []
y = []
for elem in train_data_list:
    X.append(elem[0])
    y.append(elem[1])

X = torch.stack(X)
y = torch.tensor(y)

print(X.shape, y.shape)

# X : current_dataset
# y : labels

reshaped_X = X.reshape(X.shape[0], -1)

# oversampling
oversample = RandomOverSampler()
oversampled_X, oversampled_y = oversample.fit_resample(reshaped_X, y)

# reshaping X back to the first dims
new_X = oversampled_X.reshape(-1, 32, 32, 3)

new_X = torch.tensor(new_X)
oversampled_y = torch.tensor(oversampled_y)

new_X = torch.transpose(new_X, 1, 3)
new_X = torch.transpose(new_X, 2, 3)

x_train, x_test, y_train, y_test = train_test_split(
    new_X, oversampled_y, test_size=0.3, random_state=777, stratify=oversampled_y)

# y_train = F.one_hot(y_train, 11)
# y_test = F.one_hot(y_test, 11)

# TRAIN_set = []
# for i in range(len(x_train)):
#     TRAIN_set.append((x_train[i], y_train[i].item(), 11)))

# TEST_set = []
# for i in range(len(x_test)):
#     TEST_set.append((x_test[i], torch.nn.functional.one_hot(y_test[i].item(), 11)))

# ttt_data = torch.load("train_data_tensor.pth")

BATCH_SIZE = 128
train_loader = DataLoader(
    TensorDataset(x_train, y_train), batch_size=BATCH_SIZE, shuffle=True, num_workers=2)
test_loader = DataLoader(
    TensorDataset(x_test, y_test), batch_size=len(x_test), shuffle=True, num_workers=2)

# counter = [0] * 11
# for elem in TRAIN_set:
#     counter[elem[1]] += 1
# print(counter)

# import packages for importing models

# load model and change the # of classes


class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.resnet_50 = models.resnet50(pretrained=True, progress=True)

        for layers in self.resnet_50.parameters():
            layers.requires_grad = False

        self.num_of_classes = 11  # binary classification

        self.resnet_50.fc = nn.Sequential(
            nn.Linear(2048, 1000),
            nn.ReLU(inplace=True),
            nn.Linear(1000, 256),
            nn.ReLU(inplace=True),
            nn.Linear(256, 64),
            nn.ReLU(inplace=True),
            nn.Linear(64, 11),
            nn.Softmax()
        )

        self.resnet_50.fc.requires_grad = True

    def forward(self, x):
        output = self.resnet_50(x)
        return output


wandb.login()
config = {
    'dataset': 'malware family classification',
    'model': 'resnet',
}
wandb.init(project='mfc', config=config)

from sklearn.metrics import *

def main():
    epochs = 1000
    torch.manual_seed(0)
    np.random.seed(0)
    random.seed(0)
    torch.cuda.manual_seed_all(0)

    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    model = Model().to(device)
    criterion = torch.nn.CrossEntropyLoss()
    optimizer = torch.optim.SGD(
        model.parameters(), lr=0.1, momentum=0.9, weight_decay=0.0005)
    scheduler = torch.optim.lr_scheduler.StepLR(
        optimizer, step_size=50, gamma=0.1)

    for epoch in range(epochs):
        model.train()
        scheduler.step()

        avg_loss = 0
        for step, batch in tqdm(enumerate(train_loader), total=len(train_loader)):
            mini_x, mini_y = batch
            # print(mini_y)
            mini_x = mini_x.to(device)
            mini_y = mini_y.to(device)

            pred = model(mini_x)
            # print(mini_y.shape, pred.shape)
            loss = criterion(pred, mini_y)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            avg_loss += loss / len(train_loader)

        for data, label in test_loader:
            data = data.view(-1, 3, 32, 32).to(device)

            with torch.no_grad():
                logits = model(data)

                pred = torch.argmax(logits, dim=1)

                total = len(label)
                correct = torch.eq(pred, label.to(device)).sum()

                last_accuracy = 100 * correct / total
                print("Accuracy on test set : {:.4f}%".format(
                    last_accuracy))

        wandb.log({
            'avg_loss': avg_loss,
            'acc': last_accuracy
        }, step=epoch)

        print(f'Epoch: {epoch + 1} / {epochs}. loss: {avg_loss}')


if __name__ == '__main__':
    main()
    exit()